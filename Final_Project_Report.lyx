#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass acmsiggraph
\begin_preamble
% for the following cases use the listed document class option:
% [annual] - Technical paper accepted for presentation at the ACM SIGGRAPH 
%   or SIGGRAPH Asia annual conference.
% [sponsored] - Short or full-length technical paper accepted for 
%   presentation at an event sponsored by ACM SIGGRAPH
%   (but not the annual conference Technical Papers program).
% [abstract] - A one-page abstract of your accepted content
%   (Technical Sketches, Posters, Emerging Technologies, etc.). 
%   Content greater than one page in length should use the "[sponsored]"
%   parameter.
% [preprint] - A preprint version of your final content.
% [review] - A technical paper submitted for review. Includes line
%   numbers and anonymization of author and affiliation information.

% When you submit your paper for review, please use the \TOGonlineID''
% command to include the online ID value assigned to your paper by the
% submission management system. Replace '45678' with the value you were
% assigned.
\TOGonlineid{45678}

% If you are preparing a preprint of your accepted paper, and your paper
% will be published in an issue of the ACM "Transactions on Graphics''
% journal, replace the "0'' values in the commands below with the correct
% volume and number values for that issue - you'll get them before your
% final paper is due.
\TOGvolume{0}
\TOGnumber{0}

% The TOGarticleDOI' command accepts the DOI information provided to you
% during production, and which makes up the URLs which identifies the ACM
% article page and direct PDF link in the ACM Digital Library.
% Replace "1111111.2222222'' with the values you are given.
\TOGarticleDOI{1111111.2222222}

% If you would like to include links to personal repositories for auxiliary
% material related your research contribution, you may use one or more of
% these commands to define an appropriate URL. The "\TOGlinkslist'' command
% found just before the first section of your document will add hyperlinked
% icons to your document, in addition to hyperlinked icons which point to
% the ACM Digital Library article page and the ACM Digital Library-held PDF.
\TOGprojectURL{}
\TOGvideoURL{}
\TOGdataURL{}
\TOGcodeURL{}

% Paper title.
\title{Object Tracking with Video in Real Time}

% Author and Affiliation (single author).
%\author{Name \thanks{e-mail: name@unknown.uu}\\ Research Institute}

% Author and Affiliation (multiple authors).
\author{Varun Ravishankar\thanks{e-mail: vr2263@columbia.edu}\\ Columbia University %
\and Daniel Mercado\thanks{e-mail:dm2497@columbia.edu}\\ Columbia University}

% The ``pdfauthor'' command accepts the authors of the work,
% comma-delimited, and adds this information to the PDF metadata.
\pdfauthor{Varun Ravishankar, Daniel Mercado}

% Keywords that describe your work.
\keywords{tracking, real-time, video, Lucas-Kanade, convex hulls}
\end_preamble
\options annual
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 92
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 0
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This is a template LyX file for articles to be submitted to the Special
 Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH).
 How to install the SIGGRAPH LaTeX class to your LaTeX system is explained
 in 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://wiki.lyx.org/Examples/AcmSiggraph
\end_layout

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\series bold
Note:
\series default
 The author, affiliation, email addresses, and the document title have to
 set up in the LaTeX preamble of this document!
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\series bold
Note:
\series default
 The document type (review etc.) is specified by using one of the document
 class options listed in the preamble of this file.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
maketitle 
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The command 
\series bold

\backslash
maketitle
\series default
 prepares and prints the title block.
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Design an application to perform object tracking in videos in real time.
 Given a video, detect moving objects, and track their movements.
 Tracking objects in real-time has many real-world applications.
 These include the tracking of suspicious or unusual objects, identification
 of patterns in object movement, and the general automatization of video
 analysis typically performed by human agents.
 Object tracking can be performed by identifying unique points of interest
 in each frame of the video.
 Features that do not move between frames are culled, and moving features
 are kept.
 The features grouped into objects and tracked frame by frame.
 These objects can then be classified as being of interest based on specific
 properties including: velocity, travel path, and distance between other
 objects.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
See 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://www.acm.org/class/1998/
\end_layout

\end_inset

 for details about the ACM Computing Review categories.
\end_layout

\end_inset


\end_layout

\begin_layout CR categories
\begin_inset Flex CRcat
status open

\begin_layout Plain Layout
I.4.8
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

Image Processing and Computer Vision
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

Scene Analysis
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

Tracking
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
keywordlist
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
The command 
\series bold

\backslash
keywordlist
\series default
 prints out the keywords.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
copyrightspace
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
When you use a copyright note, the 
\series bold

\backslash
copyrightspace
\series default
 command must be the first command after the start of the first section
 of the body of your paper.
 It ensures the copyright space is left at the bottom of the first column
 on the first page of your paper.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The field of computer vision provides many powerful techniques that are
 especially useful in real world applications.
 Some of these applications include: object recognition systems, gesture
 recognition, 3D object reconstruction, medical imaging, and optical character
 recognition.
 Our project focuses on locating moving objects and people from a video
 feed, using several known video tracking techniques including dense optical
 flow, background subtraction, and sparse optical flow.
 Objects can then be grouped by means of k-means clustering, convex-hulls,
 or centered-convex hulls.
\end_layout

\begin_layout Standard
The use of video tracking techniques are especially important when applied
 for security and marketing purposes.
 Changes in an objects behavior could indicate immediate danger or at least
 suspicious activity.
 The study of object paths can also benefit retail applications, by illustrating
 which in-store locations are most frequently trafficked.
 In addition to these domain specific applications, object tracking can
 aid in automating the process of video analysis that is currently performed
 by human agents.
 With websites like Youtube and Vimeo uploading thousands of hours of video
 everyday, the task of reviewing each video near impossible to perform without
 vision techniques that help with their analysis.
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Standard
blabla
\end_layout

\begin_layout Section
Technology and Development Environment
\end_layout

\begin_layout Standard
Our technology stack consisted of scripts meant to be run on POSIX machines.
 We tested and ran our scripts on Ubuntu 12.04 and OS X 10.8.3, using Python
 2.7
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.python.org/
\end_layout

\end_inset


\end_layout

\end_inset

.
 We heavily used OpenCV
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://opencv.org/
\end_layout

\end_inset


\end_layout

\end_inset

, which used NumPy
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.numpy.org/
\end_layout

\end_inset


\end_layout

\end_inset

 underneath to vectorize and accelerate matrix computations.
 We also used ffmpeg
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.ffmpeg.org/
\end_layout

\end_inset


\end_layout

\end_inset

 to allow the script to watch different video formats.
\end_layout

\begin_layout Standard
We found many of computations to be too slow for real-time usage, so we
 accelerated the computations using different libraries.
 We used Intel's Threading Building Blocks
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://threadingbuildingblocks.org/
\end_layout

\end_inset


\end_layout

\end_inset

 to allow OpenCV to be parallelized, but found that this was not enough.
 We used NVIDIA's CUDA
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.nvidia.com/object/cuda_home_new.html
\end_layout

\end_inset


\end_layout

\end_inset

 framework and Boost.Python
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.boost.org/doc/libs/1_53_0/libs/python/doc/index.html
\end_layout

\end_inset


\end_layout

\end_inset

 to create a C-extension that allowed our Python code to call OpenCV's GPU
 implementation of the feature-detection and optical flow algorithms natively.
\end_layout

\begin_layout Section
Dense Optical Flow
\end_layout

\begin_layout Standard
When first implementing our design, we initially looked at implementing
 a dense optical flow algorithm to observe motion within videos.
 Dense optical is a method that looks at two frames in a video and looks
 at every pixel in these frames.
 It then creates a motion vector between each frame for these pixels.
 Using OpenCV we implemented their Farneback algorithm
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://docs.opencv.org/modules/gpu/doc/video.html##gpu-farnebackopticalflow
\end_layout

\end_inset


\end_layout

\end_inset

 for computing dense optical flow.
 The result is an image that displays motion vectors for each frame over
 every pixel as seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dense-Optical-Flow"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename dense-optical-flow.png
	scale 35

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Dense Optical Flow
\begin_inset CommandInset label
LatexCommand label
name "fig:Dense-Optical-Flow"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As is shown, we see notice small motion vectors overlaid across the video
 image.
 Vectors with longer trails illustrate movement within in the scene.
 Although this result may look impressive, the run-time was not as fast
 we'd like it to be considering we were aiming for real-time computation.
 In addition, the sensitivity of the vectors was very low, and did not detect
 motion in the background where cars could be seen moving as well.
 As a result of these poor properties we looked to other means to implement
 our object tracking project.
\end_layout

\begin_layout Section
Background Subtraction
\end_layout

\begin_layout Standard
After attempting to implement dense optical flow and receiving poor results,
 we looked towards implementing background subtraction to isolate movement
 in our video.
 Background subtraction detects the difference between two frames and calculates
 a background estimate from the motion detected.
 The estimate is then subtracted from the frame and foreground pixels are
 left that are greater than some threshold 
\begin_inset CommandInset citation
LatexCommand cite
key "Fergus:2012uq"

\end_inset

.
 OpenCV provided us with a background subtraction algorithm
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://opencv.willowgarage.com/documentation/cpp/motion_analysis_and_object_tracki
ng.html##cv-updatemotionhistory
\end_layout

\end_inset


\end_layout

\end_inset

 to implement.
 The result of said algorithm can be in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Background-Subtraction"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename background-subtraction.png
	scale 30

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Background Subtraction
\begin_inset CommandInset label
LatexCommand label
name "fig:Background-Subtraction"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Detection for large vehicles worked very well when using background subtraction.
 In addition, the speed of the method was near run-time and great for what
 we were looking for in the application.
 However, background subtraction suffered from one of the same problems
 that dense optical flow suffered from.
 Mainly it suffered from its sensitivity and inability to track smaller
 objects moving in the background of the video.
 One can modify the threshold from which image binarization occurs to help
 with detect smaller objects in the video.
 However, we one does this they also tend to pick up background noise found
 in the video stream.
 This noise makes it difficult to differentiate between real-world objects
 in motion and perceived motion.
\end_layout

\begin_layout Section
Sparse Optical Flow
\end_layout

\begin_layout Standard
After reviewing our results from background subtraction we looked in computing
 methods that implement sparse optical flow.
 Sparse optical flow does not look at every pixel in a frame and instead
 looks at specific locations in the image to track between frames.
 The locations or features
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://docs.opencv.org/modules/imgproc/doc/feature_detection.html##goodfeaturestotr
ack
\end_layout

\end_inset


\end_layout

\end_inset

 are chosen based on their uniqueness in terms of texture and intensity.
 The Lucas-Kanade method 
\begin_inset CommandInset citation
LatexCommand cite
key "Lucas_1981_2548"

\end_inset

 takes these features and estimates the motion in the frame based on the
 change of intensity of pixels in the immediate area of the features 
\begin_inset CommandInset citation
LatexCommand cite
key "Rojas:fk"

\end_inset

.
 We also chose to use further refine our features by finding sub-pixel locations
 of the corners.
 We used OpenCV's implementation of Lucas-Kanade optical flow
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html##
calcopticalflowpyrlk
\end_layout

\end_inset


\end_layout

\end_inset

, as well as their GoodFeaturesToTrack
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://docs.opencv.org/modules/imgproc/doc/feature_detection.html##goodfeaturestotr
ack
\end_layout

\end_inset


\end_layout

\end_inset

 method to provide us with very strong features for tracking.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Sparse-Optical-Flow"

\end_inset

, we can see these features and the trails they leave when computing sparse
 optical flow.
 The computation runs in real-time and is quite accurate at detecting objects
 that are in motion.
 However, there are two flaws to this method.
 The first the even features that are not in motion are displayed.
 The second problem is that features are recomputed every frame.
 The lead to features being dragged of the screen, or having large moving
 objects not detected at all.
 The frames per second achieved with the Lucas-Kanade algorithm was very
 strong, and for the most part when feature were re-generated regularly
 we found that sparse optical provided good movement tracking in real-time
 for tracking objects.
 We found that regenerating features every 5 frames gave us features frequent
 enough that no objects were missed when performing tracking.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename sparse-optical-flow.png
	scale 30

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Background Subtraction
\begin_inset CommandInset label
LatexCommand label
name "fig:Sparse-Optical-Flow"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Grouping
\end_layout

\begin_layout Standard
Finding features to track does not alway mean finding objects.
 Pixels not in the foreground may be selected as good potential features
 based on their uniqueness, not on movement or presence in the foreground.
 In order to resolve this issue it was necessary to group these features
 on some heuristic to see objects moving in the video.
 This particular portion of the application difficult to resolve during
 development.
 We chose to implement three different strategies when grouping features
 and 
\end_layout

\begin_layout Subsection
K-Means Clustering
\end_layout

\begin_layout Standard
What is it?
\end_layout

\begin_layout Standard
How does the algorithm work?
\end_layout

\begin_layout Standard
What library/implementation was used?
\end_layout

\begin_layout Standard
What were the results? Good? Bad?
\end_layout

\begin_layout Standard
Why wasn't it good enough?
\end_layout

\begin_layout Subsection
Convex Hulls
\end_layout

\begin_layout Standard
What is it?
\end_layout

\begin_layout Standard
How does the algorithm work?
\end_layout

\begin_layout Standard
What library/implementation was used?
\end_layout

\begin_layout Standard
What were the results? Good? Bad?
\end_layout

\begin_layout Standard
Why wasn't it good enough?
\end_layout

\begin_layout Subsection
Centered Convex Hulls
\end_layout

\begin_layout Standard
What is it?
\end_layout

\begin_layout Standard
How does the algorithm work?
\end_layout

\begin_layout Standard
What library/implementation was used?
\end_layout

\begin_layout Standard
What were the results? Good? Bad?
\end_layout

\begin_layout Standard
Why wasn't it good enough?
\end_layout

\begin_layout Section
Improving Performance with CUDA and Finding Other Bottlenecks
\end_layout

\begin_layout Standard
After creating centered convex hulls, we realized that our implementation's
 performance was too slow for real-time usage.
 We decided to focus on improving real-time performance by moving much of
 the work being done in Python to native code.
 When even this did not prove to be enough, we decided to move as much code
 as possible to the GPU.
 OpenCV contains a set of classes that utilize the GPU, taking advantage
 of the high levels of multi-parallelism available to GPUs.
 OpenCV contains two different implementations of common computer vision
 algorithms.
 One works on OpenCL, meant to work in heterogenous computing environments.
 The other works only on NVIDIA GPUs and uses the CUDA Runtime API.
 The CUDA implementation is more fleshed-out than the OpenCL implementation
 and so we decided to focus on this version.
 However, there were many technical issues with this approach.
\end_layout

\begin_layout Standard
We first learned to call native code from Python using Boost.Python.
 This involved writing a file that told Boost what methods to make available
 to Python and what the arguments to these methods were.
 We next wrote the accompanying C code that called the OpenCV functions
 implementing the functionality we wanted.
 OpenCV uses NumPy arrays in its Python implementation, so we first needed
 to convert the NumPy arrays into OpenCV matrix objects.
 This involved finding the correct conversion function in the OpenCV code
 and converting it so that it fit the method's parameters.
 If there was no conversion method already available, we built our own function
 to unpackage the Python types into corresponding C++ objects.
 We then converted the OpenCV matrix objects into the special type needed
 for OpenCV's GPU matrix types.
 These types differed mostly in that the GPU version allocated memory on
 the GPU and would be cleaned up when the deconstructor was called.
 We next called the GPU code, unpackaged the resulting GPU objects into
 OpenCV native types, and then packaged the C++ objects into corresponding
 Python objects.
\end_layout

\begin_layout Standard
After creating our C extension, we integrated the native module into our
 Python code.
 We found that performance improved by roughly 200% after doing this, as
 measured by the frame rate.
 Before implementing the CUDA code, our frame rate was roughly 2 FPS, and
 after implementing the CUDA code, our frame rate rose to 4 FPS.
 This was still not enough for real-time usage, so we decided to implement
 multithreading to distribute some of the work still on the CPU onto multiple
 cores.
 We used Python's multiprocessing module to distribute the code that checked
 if a feature was in a hull across multiple cores.
 Python does not have full support for threads because of its Global Interpreter
 Lock (GIL), so it instead creates multiple processes that communicate back
 and forth using pipes or sockets.
 This has some overhead, but is still an easy way to distribute work over
 multiple cores.
 Doing this and testing our new implementation on 8 cores, we found that
 the frame rate increased to between 4.5-5 FPS, an improvement between 50-100%.
\end_layout

\begin_layout Standard
We next profiled our code further to detect bottlenecks.
 We had moved as much Python code to native or GPU code as possible, and
 found that the profiler confirmed this, as seen in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Python-Profiler-Output"

\end_inset

.
\end_layout

\begin_layout Standard
The 'acquire' method arises because of Python's multithreading.
 This is not really lock-contention, but it does indicate that Python is
 spending most of its time in the multiprocessing module, managing the different
 processes.
 Most time is being spent in the work being done in the thread pool, which
 consists of finding the closest hull to the subset of points handed to
 each process.
 Because this work was being done across multiple processes and the method
 being called was written in C, we determined that we could not improve
 performance anymore; there was an algorithmic bottleneck here that prevented
 further improvement.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
In the end, we were able to improve our tracking system and make it performant
 enough to use for a human to use it in real-time.
 Our implementation focused on improving performance, and we looked at different
 algorithms for optical flow to do this.
 We ruled out dense optical flow and background subtraction after determining
 that both approaches led to too much noise and were unable to detect motion
 in the background of the video.
 After experimenting with different optical flow algorithms and grouping
 mechanisms, we narrowed down on sparse optical flow with Lucas-Kanade and
 on creating clusters by finding convex hulls.
 This combination was performant and was able to detect small objects in
 the background.
 However, when grouping objects, we ran into performance problems.
 We were able to increase frame rates by moving some of the work into the
 GPU and by distributing the work across multiple cores, but ran into algorithmi
c bounds on the number of points and hulls that must be checked to group
 features.
\end_layout

\begin_layout Standard
Future work can be done with this system to improve performance by doing
 background subtraction before convex hulls are found.
 This would reduce the number of hulls and decrease the number of hulls
 that must be checked per point.
 Furthermore, more work can be done in distributing the convex hull checks
 across multiple cores.
 Python's implementation of multiprocessing is easy to use, but can lead
 to poor performance and lock contention.
 This would require moving much of the multiprocessing implementation into
 C++ and using locks and threads instead of communicating processes.
\end_layout

\begin_layout Standard
To make the system useful to civilian and military applications, it is necessary
 to determine criteria for what makes an object suspicious.
 We began work on determining vectors and movement patterns for objects
 but were unable to finish this.
 Once movement patterns are detected, work can be done in determining if
 a given pattern is out of the ordinary e.g.
 one object moves in the opposite direction of the others.
 For law enforcement applications, this would enable the system to detect
 suspicious objects, such as criminals or enemy combatants and alert the
 authorities of such behavior.
 For commercial applications, this could help function as a more sophisticated
 security system.
 Instead of simply detecting motion and turning on a light or an alarm,
 the system could determine if such motion is out of the ordinary and has
 not occurred before; if that is the case, then a security company can be
 contacted or the building owner can be contacted.
\end_layout

\begin_layout Section*
Acknowledgements
\end_layout

\begin_layout Standard
Thanks to Austin Reiter for his guidance in this project.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Final_Project_Report"
options "acmsiggraph"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Section
\start_of_appendix
Python Profiler Output for CUDA and Multiprocessing Implementation
\begin_inset CommandInset label
LatexCommand label
name "sec:Python-Profiler-Output"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,tabsize=4"
inline false
status open

\begin_layout Plain Layout

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
\end_layout

\begin_layout Plain Layout

1879   45.466    0.024   45.466    0.024 {method 'acquire' of 'thread.lock' objects}
\end_layout

\begin_layout Plain Layout

266   13.206    0.050   13.206    0.050 {cv2.waitKey}
\end_layout

\begin_layout Plain Layout

265    7.844    0.030    9.828    0.037 lktrack.py:83(track_points)
\end_layout

\begin_layout Plain Layout

145334    6.894    0.000    6.894    0.000 {cv2.convexHull}
\end_layout

\begin_layout Plain Layout

266    6.539    0.025    6.539    0.025 {method 'detect' of 'cv2.MSER' objects}
\end_layout

\begin_layout Plain Layout

1    2.607    2.607    2.610    2.610 lktrack.py:65(detect_points)
\end_layout

\begin_layout Plain Layout

530    1.636    0.003    1.636    0.003 {cv2.calcOpticalFlowPyrLK}
\end_layout

\begin_layout Plain Layout

266    1.097    0.004   60.449    0.227 lktrack.py:151(draw)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
